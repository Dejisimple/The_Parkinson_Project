---
---
Title: "Parkinson’s Disease Prediction using Metabolomics"
Author: Oladimeji Adefila
University of Michigan–Flint · Group 1


Output:
  html_document:
    theme: default
    toc: true
    toc_float: true

Date: "2025-10-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


----------------------------------------------------------------------- ----------
This section installs and loads all required packages for the project.
Run the install commands only once on a new system; afterwards, you can comment them out.
--------------------------------------------------------------------------------------


```{r}
# Install required packages (first time only)

# install.packages(c("readxl","tidyverse","caret","pROC","randomForest","glmnet","gridExtra"))

# Load core libraries

library(readxl)
library(tidyverse)
library(caret)
library(pROC)
library(randomForest)
library(glmnet)
library(gridExtra)

# Reproducibility

set.seed(123)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```



Data Import and Overview
---------------------------------------------------------------------------------
In this step we load both the Sample Meta Data and Log Transformed Data sheets from the provided Excel file, then explore their structure.
------------------------------------------ ---------------------------------------


```{r}
meta <- read_excel("C:/Users/HOME/Downloads/My FORD-0101-21ML+ DATA TABLES_SERUM (METADATA UPDATE) - 5.XLSX.xlsx",
sheet = "Sample Meta Data")

data_log <- read_excel("C:/Users/HOME/Downloads/My FORD-0101-21ML+ DATA TABLES_SERUM (METADATA UPDATE) - 5.XLSX.xlsx",
sheet = "Log Transformed Data")

# Preview metadata and metabolite table

head(meta)
head(data_log[, 1:10])

```




----------------------------------------------------------------------------------
Target Variable and Merge

We identify the diagnostic column in the metadata, recode it as a binary target (PD vs Control), and merge it with the metabolite table.
--------------------------------------------------------------------------------


```{r}
meta <- meta %>%
mutate(DIAGNOSIS = ifelse(COHORT == "PPMI", "PD", "Control"))

merged_data <- meta %>%
select(PARENT_SAMPLE_NAME, DIAGNOSIS) %>%
inner_join(data_log, by = "PARENT_SAMPLE_NAME")

table(merged_data$DIAGNOSIS)
dim(merged_data)

```



------------------------------------------------------------------------------
Data Normalization and Cleaning

We scale numeric features (Z-score) and ensure no missing or infinite values remain.

-----------------------------------------------------------------------------


```{r}
X <- merged_data %>% select(-PARENT_SAMPLE_NAME, -DIAGNOSIS)
X_scaled <- as.data.frame(scale(X))

final_data <- cbind(DIAGNOSIS = merged_data$DIAGNOSIS, X_scaled)

# Replace Inf/NaN with median per column

X_num <- final_data %>% select(-DIAGNOSIS)
X_num[!is.finite(as.matrix(X_num))] <- NA
for (col in colnames(X_num)) {
X_num[[col]][is.na(X_num[[col]])] <- median(X_num[[col]], na.rm = TRUE)
}
final_data <- cbind(DIAGNOSIS = final_data$DIAGNOSIS, X_num)

sum(is.na(final_data))

```



------------------------------------------------------------------- -----------------------
We use correlation filtering to remove highly correlated variables (cutoff = 0.9) and apply LASSO regression for dimensionality reduction.
---------------------------------------------------------------------------------

-------------------
Correlation Filter
-------------------

```{r}
# --- Correlation Filter ---

corr_matrix <- cor(final_data %>% select(-DIAGNOSIS), use = "pairwise.complete.obs")
corr_matrix[is.na(corr_matrix)] <- 0
high_corr <- findCorrelation(corr_matrix, cutoff = 0.9)

filtered_data <- final_data %>%
select(DIAGNOSIS, -all_of(colnames(final_data %>% select(-DIAGNOSIS))[high_corr]))

cat("✅ Features before filtering:", ncol(final_data) - 1, "\n")
cat("✅ Features after filtering:", ncol(filtered_data) - 1, "\n")

```



------------------------
LASSO Feature Selection
-----------------------
```{r}
# --- LASSO Feature Selection ---

X_matrix <- as.matrix(filtered_data %>% select(-DIAGNOSIS))
y_numeric <- ifelse(filtered_data$DIAGNOSIS == "PD", 1, 0)

cv.lasso <- cv.glmnet(X_matrix, y_numeric, family = "binomial", alpha = 1)
plot(cv.lasso)
title("LASSO Cross-Validation Curve", line = 2.5)

lasso_coef <- coef(cv.lasso, s = "lambda.min")
selected_features <- rownames(lasso_coef)[lasso_coef[, 1] != 0][-1]

lasso_data <- filtered_data %>%
select(DIAGNOSIS, all_of(selected_features))

cat("✅ Number of LASSO-selected features:", length(selected_features), "\n")

```


---------------------------------------------------------------------------
We split the dataset (80/20) for both the full and LASSO-reduced features.
----------------------------------------------------------------------------


```{r}
# Split train/test safely
train_index <- createDataPartition(filtered_data$DIAGNOSIS, p = 0.8, list = FALSE)

train_full  <- dplyr::slice(filtered_data, train_index)
test_full   <- dplyr::slice(filtered_data, -train_index)
train_lasso <- dplyr::slice(lasso_data, train_index)
test_lasso  <- dplyr::slice(lasso_data, -train_index)

# Encode target
train_full$DIAGNOSIS  <- factor(train_full$DIAGNOSIS,  levels = c("Control", "PD"))
test_full$DIAGNOSIS   <- factor(test_full$DIAGNOSIS,   levels = c("Control", "PD"))
train_lasso$DIAGNOSIS <- factor(train_lasso$DIAGNOSIS, levels = c("Control", "PD"))
test_lasso$DIAGNOSIS  <- factor(test_lasso$DIAGNOSIS,  levels = c("Control", "PD"))


```

    Model Building
----------------------------------------------------------------------------------------
We build two models: Logistic Regression and Random Forest, using both full and LASSO datasets.
-------------------------------------------------------------------------------------

Logistic Regression

```{r}
# Logistic Regression

model_lr_full <- glm(DIAGNOSIS ~ ., data = train_full, family = binomial)
model_lr_lasso <- glm(DIAGNOSIS ~ ., data = train_lasso, family = binomial)

pred_lr_full  <- predict(model_lr_full,  test_full,  type = "response")
pred_lr_lasso <- predict(model_lr_lasso, test_lasso, type = "response")

pred_class_lr_full  <- ifelse(pred_lr_full  > 0.5, "PD", "Control")
pred_class_lr_lasso <- ifelse(pred_lr_lasso > 0.5, "PD", "Control")

```


---------------------------------
Random Forest
---------------------------


```{r}
library(randomForest)

# Fix column names first
colnames(train_full)  <- make.names(colnames(train_full), unique = TRUE)
colnames(test_full)   <- make.names(colnames(test_full), unique = TRUE)
colnames(train_lasso) <- make.names(colnames(train_lasso), unique = TRUE)
colnames(test_lasso)  <- make.names(colnames(test_lasso), unique = TRUE)
```


```{r}
# Define predictor sets safely
predictors_full  <- setdiff(names(train_full),  "DIAGNOSIS")
predictors_lasso <- setdiff(names(train_lasso), "DIAGNOSIS")
```


```{r}
# Ensure predictors exist in test dataset
predictors_lasso <- intersect(predictors_lasso, names(test_lasso))
predictors_full  <- intersect(predictors_full,  names(test_full))
```


```{r}
# --- Random Forest (Full Feature Set)
if (length(predictors_full) > 0) {
  model_rf_full <- randomForest(
    x = train_full[, predictors_full],
    y = train_full$DIAGNOSIS,
    importance = TRUE
  )
  pred_rf_full <- predict(model_rf_full, test_full[, predictors_full])
}
```


```{r}
# --- Random Forest (LASSO Feature Set)
if (length(predictors_lasso) > 0) {
  model_rf_lasso <- randomForest(
    x = train_lasso[, predictors_lasso],
    y = train_lasso$DIAGNOSIS,
    importance = TRUE
  )
  pred_rf_lasso <- predict(model_rf_lasso, test_lasso[, predictors_lasso])
}

```




     Model Evaluation
-------------------------------------------------------------------------------
We compute ROC curves, AUC values, and display confusion matrices for each model.
--------------------------------------------------------------------------------



```{r}
roc_lr_full   <- roc(as.numeric(test_full$DIAGNOSIS == "PD"),  pred_lr_full)
roc_lr_lasso  <- roc(as.numeric(test_lasso$DIAGNOSIS == "PD"), pred_lr_lasso)
roc_rf_full   <- roc(as.numeric(test_full$DIAGNOSIS == "PD"),  as.numeric(pred_rf_full == "PD"))
roc_rf_lasso  <- roc(as.numeric(test_lasso$DIAGNOSIS == "PD"), as.numeric(pred_rf_lasso == "PD"))

auc_results <- data.frame(
Model = c("LR Full","LR LASSO","RF Full","RF LASSO"),
AUC   = c(auc(roc_lr_full), auc(roc_lr_lasso), auc(roc_rf_full), auc(roc_rf_lasso))
)

auc_results

```



Visualization Dashboard
------------------------------------------------------------------------------------------
This section visualizes model performance and feature importance in a single 2×2 dashboard.
-------------------------------------------------------------------------------------------

```{r}
# ROC Curves

p1 <- ggroc(list("LR Full" = roc_lr_full, "LR LASSO" = roc_lr_lasso,
"RF Full" = roc_rf_full, "RF LASSO" = roc_rf_lasso)) +
geom_line(size = 1.2) +
scale_color_manual(values = c("blue","green","red","purple")) +
labs(title = "ROC Curves Comparison", x = "1 - Specificity", y = "Sensitivity") +
theme_minimal()
```


```{r}
# AUC Comparison

p2 <- ggplot(auc_results, aes(Model, AUC, fill = Model)) +
geom_col(width = 0.6) +
geom_text(aes(label = round(AUC, 3)), vjust = -0.5, size = 4) +
ylim(0, 1) +
labs(title = "Model AUC Comparison", y = "AUC", x = "Model Type") +
scale_fill_manual(values = c("blue","green","red","purple")) +
theme_minimal()
```


```{r}
# Feature Importance

importance_df <- as.data.frame(importance(model_rf_lasso))
importance_df$Metabolite <- rownames(importance_df)
importance_top20 <- importance_df %>%
arrange(desc(MeanDecreaseGini)) %>% head(20)

p3 <- ggplot(importance_top20, aes(x = reorder(Metabolite, MeanDecreaseGini),
y = MeanDecreaseGini, fill = MeanDecreaseGini)) +
geom_col() + coord_flip() +
labs(title = "Top 20 Metabolites Driving PD Classification",
x = "Metabolite (m/z Feature)", y = "Importance (Gini)") +
theme_minimal()
```


--------------------------
ROC Curves Comparison
------------------------------

```{r}
# ROC Curves Comparison
p1 <- ggroc(list(
  "LR Full"   = roc_lr_full,
  "LR LASSO"  = roc_lr_lasso,
  "RF Full"   = roc_rf_full,
  "RF LASSO"  = roc_rf_lasso
)) +
  geom_line(size = 1.2) +
  scale_color_manual(values = c("blue", "green", "red", "purple")) +
  labs(
    title = "ROC Curves Comparison",
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal()
```


```{r}
# Show only the ROC plot
p1

```




-----------------------------
Model AUC Comparison
-----------------------------

```{r}
p2 <- ggplot(auc_results, aes(x = Model, y = AUC, fill = Model)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = round(AUC, 3)), vjust = -0.5, size = 4) +
  ylim(0, 1) +
  labs(
    title = "Model AUC Comparison",
    y = "Area Under Curve (AUC)",
    x = "Model Type"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("blue", "green", "red", "purple"))
```


```{r}
# Show only the AUC bar chart
p2

```


---------------------------------------------
Top 20 Metabolites (Feature Importance)
-----------------------------------------------
```{r}
importance_df <- as.data.frame(importance(model_rf_lasso))
importance_df$Metabolite <- rownames(importance_df)

importance_top20 <- importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  head(20)

p3 <- ggplot(importance_top20, aes(
  x = reorder(Metabolite, MeanDecreaseGini),
  y = MeanDecreaseGini,
  fill = MeanDecreaseGini
)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Top 20 Metabolites Driving PD Classification",
    x = "Metabolite (m/z Feature)",
    y = "Mean Decrease Gini (Importance)"
  ) +
  theme_minimal()
```


```{r}
# Show only the feature importance chart
p3

```

----------------------------------
Confusion Matrix Heatmap
--------------------------------
```{r}
cm_rf_lasso <- confusionMatrix(pred_rf_lasso, test_lasso$DIAGNOSIS)
cm_df <- as.data.frame(cm_rf_lasso$table)

p4 <- ggplot(cm_df, aes(Reference, Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "white", size = 5, fontface = "bold") +
  scale_fill_gradient(low = "#c0392b", high = "#27ae60") +
  labs(
    title = "Confusion Matrix – Random Forest (LASSO)",
    x = "Actual Class",
    y = "Predicted Class"
  ) +
  theme_minimal()
```


```{r}
# Show only the confusion matrix plot
p4

```


```{r}
grid.arrange(p1, p2, p3, p4, ncol = 2)

```



```{r}
# Confusion Matrix

cm_rf_lasso <- confusionMatrix(pred_rf_lasso, test_lasso$DIAGNOSIS)
cm_df <- as.data.frame(cm_rf_lasso$table)

p4 <- ggplot(cm_df, aes(Reference, Prediction, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), color = "white", size = 5, fontface = "bold") +
scale_fill_gradient(low = "#c0392b", high = "#27ae60") +
labs(title = "Confusion Matrix – Random Forest (LASSO)",
x = "Actual Class", y = "Predicted Class") +
theme_minimal()

grid.arrange(p1, p2, p3, p4, ncol = 2)

```

----------------------------------------
We Save Visual Outputs
--------------------------------------

```{r}
if(!dir.exists("PD_Visuals")) dir.create("PD_Visuals")

save_plot <- function(plot, filename, width=7, height=5, dpi=400) {
ggsave(filename = paste0("PD_Visuals/", filename, ".png"),
plot = plot, width = width, height = height, dpi = dpi)
}

save_plot(p1, "ROC_Curves_Comparison")
save_plot(p2, "Model_AUC_Comparison")
save_plot(p3, "Top20_Metabolites_Importance")
save_plot(p4, "Confusion_Matrix_RF_LASSO")

png("PD_Visuals/Full_Dashboard.png", width=1500, height=1000, res=200)
grid.arrange(p1, p2, p3, p4, ncol=2)
dev.off()
cat("✅ All plots saved to the folder: PD_Visuals\n")

```





# Discussion

From the evaluation results:

 1, Both Logistic Regression and Random Forest performed well, but Random Forest with   
    LASSO-selected   features achieved the highest AUC. 
    
 2, The selected metabolites show strong discriminatory power between PD and control groups.
 

 3, These findings suggest that metabolomics-based models can support early, non-invasive PD     
    detection.  
    
    
 4,  Future work can include larger cohorts and validation on independent datasets.

```



               END                END                END











```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
